{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9706785c-9ae1-49c1-bfd2-24957bfb6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## required libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff065c3-7f9c-46a2-9811-48985c578263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import logging\n",
    "from entmax import sparsemax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce8f75e-2640-4bb3-aac9-51a399e5e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "# Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e601598-9124-4a67-aacf-9930847eef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.basicConfig(filename='outliers.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9179f978-e2b3-4d38-9e74-7919695de677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "545d9483-d5d2-4230-8cdc-e71b5b71a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_paths, seq_len=10, date_column='Date', feature_columns=['Open', 'Close', 'High', 'Low']):\n",
    "    dfs = []\n",
    "    for file in file_paths:\n",
    "        df = pd.read_excel(file)\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        required_columns = [date_column.lower()] + [col.lower() for col in feature_columns]\n",
    "\n",
    "        def convert_date(date):\n",
    "            if isinstance(date, (int, float)):\n",
    "                base_date = datetime(1899, 12, 30)\n",
    "                return pd.to_datetime(base_date + timedelta(days=date))\n",
    "            elif isinstance(date, pd.Timestamp):\n",
    "                return date\n",
    "            else:\n",
    "                return pd.to_datetime(date, errors='coerce')\n",
    "\n",
    "        df['date'] = df['date'].apply(convert_date)\n",
    "        df = df.set_index('date')\n",
    "\n",
    "        for col in feature_columns:\n",
    "            df[col.lower()] = df[col.lower()].round(2)\n",
    "\n",
    "        if 'volume' in df.columns:\n",
    "            df = df[df['volume'] != 0]\n",
    "            df = df.drop(columns=['volume'])\n",
    "\n",
    "        df = df[required_columns[1:]].dropna()\n",
    "        dfs.append(df)\n",
    "\n",
    "    common_dates = pd.concat([df[['close']] for df in dfs], axis=1).dropna().index\n",
    "    dfs = [df.loc[common_dates] for df in dfs]\n",
    "\n",
    "    data = np.stack([df[[col.lower() for col in feature_columns]].values for df in dfs], axis=1)\n",
    "    sequences = [data[i:i + seq_len] for i in range(len(data) - seq_len + 1)]\n",
    "    sequences = torch.tensor(np.array(sequences), dtype=torch.float32).to(device)\n",
    "    print(f\"Final sequence tensor shape: {sequences.shape}\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a99e1a00-1454-4bed-acde-37ff02c2f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dilated Causal Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d846c6f0-bb91-4de8-be0c-11a950d8cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
    "        super(DCC, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                              padding=(kernel_size - 1) * dilation, dilation=dilation)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        padding = (self.kernel_size - 1) * self.dilation\n",
    "        x = x[:, :, :-padding] if padding > 0 else x\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26e3ddc9-e132-4aeb-8e50-bd39bdab585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5049882b-8590-4591-b42e-075d4914e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.1):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.Wg = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.Wa = nn.Linear(2 * in_features, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, node_features, edge_info):\n",
    "        batch_size, num_nodes, in_features = node_features.size()\n",
    "        h = self.Wg(node_features)\n",
    "\n",
    "        attention = torch.zeros(batch_size, num_nodes, num_nodes, device=node_features.device)\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                concat_features = torch.cat([node_features[:, i, :], node_features[:, j, :]], dim=-1)\n",
    "                gate = torch.tanh(self.Wa(concat_features))\n",
    "                attention[:, i, j] = edge_info[:, i, j] * gate.squeeze()\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        h_prime = torch.bmm(attention, h)\n",
    "        return F.elu(h_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baf5ea33-6594-4c80-9999-5585c0990c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio Optimization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6472b06a-b2a7-4ba5-8c9d-076a1a455fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioOptimization(nn.Module):\n",
    "    def __init__(self, num_assets, num_features=4, hidden_dim=64, input_seq_len=9):\n",
    "        super(PortfolioOptimization, self).__init__()\n",
    "        self.num_assets = num_assets\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_seq_len = input_seq_len\n",
    "        \n",
    "        self.dcc1 = DCC(num_features, hidden_dim, kernel_size=3, dilation=1)\n",
    "        self.dcc2 = DCC(hidden_dim, hidden_dim, kernel_size=3, dilation=2)\n",
    "        self.dcc3 = DCC(hidden_dim, hidden_dim, kernel_size=3, dilation=4)\n",
    "\n",
    "        self.Wq = nn.Linear(num_features * input_seq_len, hidden_dim)\n",
    "        self.Wk = nn.Linear(num_features * input_seq_len, hidden_dim)\n",
    "        self.Wv = nn.Linear(num_features * input_seq_len, hidden_dim)\n",
    "\n",
    "        self.gat = GATLayer(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.Wr = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.We = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dcc_pred = DCC(hidden_dim * 3, num_features, kernel_size=3, dilation=1)\n",
    "\n",
    "        self.Wf = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.conv_policy = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.Wt = nn.Linear(hidden_dim + num_features, hidden_dim)\n",
    "        self.Ww = nn.Linear(hidden_dim, num_assets)\n",
    "\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.tensor(.5))  # Learnable scalar\n",
    "\n",
    "\n",
    "    def forward(self, x, prev_weights=None):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        batch_size, seq_len, num_assets, num_features = x.size()\n",
    "\n",
    "        x_reshaped = x.permute(0, 2, 3, 1).reshape(batch_size * num_assets, num_features, seq_len)\n",
    "        fe = self.dcc1(x_reshaped)\n",
    "        fe = self.dcc2(fe)\n",
    "        fe = self.dcc3(fe)\n",
    "        fe = fe[:, :, -1].reshape(batch_size, num_assets, self.hidden_dim)\n",
    "\n",
    "        patches_flat = x.reshape(batch_size, num_assets, -1)\n",
    "        q = self.Wq(patches_flat)\n",
    "        k = self.Wk(patches_flat)\n",
    "        v = self.Wv(patches_flat)\n",
    "        attention = torch.bmm(q, k.transpose(1, 2)) / (self.hidden_dim ** 0.5)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "\n",
    "        fr = self.gat(fe, attention)\n",
    "\n",
    "        fm = self.Wr(fr) + self.We(fe)\n",
    "        fm = fm.mean(dim=1, keepdim=True).expand(-1, num_assets, -1)\n",
    "\n",
    "        f = torch.cat([fe, fr, fm], dim=-1)\n",
    "        f_reshaped = f.permute(0, 2, 1)\n",
    "        x_pred = self.dcc_pred(f_reshaped)\n",
    "        x_pred = x_pred.permute(0, 2, 1)\n",
    "\n",
    "        f_policy = F.relu(self.conv_policy(self.Wf(f).permute(0, 2, 1))).permute(0, 2, 1)\n",
    "        f_policy = self.Wt(torch.cat([f_policy, x_pred], dim=-1))\n",
    "        \n",
    "        if prev_weights is not None:\n",
    "            f_policy = f_policy + prev_weights.unsqueeze(-1)\n",
    "        \n",
    "        # Select last time step for weight generation\n",
    "        raw_weights = self.Ww(f_policy[:, -1]) / self.temperature\n",
    "        raw_scores = self.Ww(f_policy[:, -1])  # [batch_size, num_assets]\n",
    "\n",
    "        # weights = F.softmax(raw_weights, dim=-1)\n",
    "        # weights = 0.05 + 0.80 * weights  # 5% min, 95% max per asset\n",
    "        weights = F.softmax(raw_weights / (self.temperature + 1e-8), dim=-1)  # Add temperature\n",
    "        weights = 0.10 + 0.60 * weights  # New: 10% min, 70% max (tighter bounds)\n",
    "        weights = weights / torch.sum(weights, dim=-1, keepdim=True)  # Renormalize\n",
    "\n",
    "        \n",
    "        return x_pred, weights\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Loss Functions\n",
    "# ------------------------\n",
    "def prediction_loss(pred, target):\n",
    "    return F.mse_loss(pred, target)\n",
    "\n",
    "def transaction_cost(prev_weights, new_weights, cost_rate=0.009):\n",
    "    if prev_weights is None:\n",
    "        return torch.zeros(new_weights.size(0), device=new_weights.device)\n",
    "    return cost_rate * torch.sum(torch.abs(new_weights - prev_weights), dim=-1)\n",
    "def entropy_loss(weights):\n",
    "    return -torch.sum((weights+ 1e-6) * torch.log(weights + 1e-8), dim=-1)  # per sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4ad202f-411a-499a-871a-d05b65921767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59555749-3524-43e5-a460-5d1435648cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, sequences, num_epochs=15, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    prev_weights = None\n",
    "    for epoch in range(num_epochs):\n",
    "        total_pred_loss = 0\n",
    "        total_reward = 0\n",
    "        for batch in sequences:\n",
    "            x_input = batch[:-1].unsqueeze(0)\n",
    "            target = batch[1:].unsqueeze(0)[:, -1]\n",
    "\n",
    "            x_pred, weights = model(x_input, prev_weights)\n",
    "            pred_loss = prediction_loss(x_pred, target)\n",
    "\n",
    "            relative_prices = target[:, :, 1] / batch[-2, :, 1]\n",
    "            portfolio_return = torch.sum(relative_prices * weights.squeeze(1), dim=-1)\n",
    "            transaction_costs = transaction_cost(prev_weights, weights.squeeze(1), cost_rate=0.0003)\n",
    "            u_t = 1 - transaction_costs\n",
    "            log_return = torch.log(u_t * portfolio_return + 1e-8)\n",
    "            entropy = entropy_loss(weights.squeeze(1))\n",
    "\n",
    "\n",
    "            lambda_entropy = 100 # tune this value\n",
    "            portfolio_variance = torch.var(relative_prices * weights.squeeze(1))\n",
    "            lambda_variance = 0.6  # Risk-return tradeoff\n",
    "\n",
    "            hhi_penalty = torch.sum(weights**2)  # Measures concentration\n",
    "            lambda_hhi = 1.0  # Strength of HHI penalty\n",
    "\n",
    "            rl_loss = (-torch.mean(log_return) + lambda_entropy * torch.mean(entropy) \n",
    "                       + lambda_variance * portfolio_variance+ lambda_hhi * hhi_penalty)  # New penalty\n",
    "            #rl_loss = -torch.mean(log_return) + lambda_entropy * torch.mean(entropy) + lambda_variance * portfolio_variance            \n",
    "            \n",
    "            lambda_pred = 0.1  # Prediction loss weight\n",
    "            loss = lambda_pred * pred_loss + rl_loss  # Focus on returns/risk\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_pred_loss += pred_loss.item()\n",
    "            total_reward += log_return.mean().item()\n",
    "            prev_weights = weights.detach().squeeze(1)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Pred Loss: {total_pred_loss / len(sequences):.4f}, Avg Return: {total_reward / len(sequences):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f65ad24-f8fc-4a7c-849f-8cd03f484f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b96b58ee-20d5-47a9-9a66-1ab8007ba1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sequence tensor shape: torch.Size([831, 10, 6, 4])\n",
      "Epoch 1, Pred Loss: 7828.3176, Avg Return: 0.0018\n",
      "Epoch 2, Pred Loss: 7738.6075, Avg Return: 0.0018\n",
      "Epoch 3, Pred Loss: 7733.0211, Avg Return: 0.0018\n",
      "Epoch 4, Pred Loss: 7733.7358, Avg Return: 0.0018\n",
      "Epoch 5, Pred Loss: 7729.0688, Avg Return: 0.0018\n",
      "Predicted Prices (Open, Close, High, Low): [[[165.38034    0.       168.20473  164.30557 ]\n",
      "  [179.40688    0.       183.73047  178.32265 ]\n",
      "  [ 36.527283   0.        39.276367  37.80002 ]\n",
      "  [116.40097    0.       119.67396  116.74182 ]\n",
      "  [ 26.487701   0.        28.22962   27.306604]\n",
      "  [ 66.38871    0.        68.580635  67.022   ]]]\n",
      "Portfolio Weights: [0.08333332 0.08333332 0.08333332 0.08333332 0.5833333  0.08333332]\n",
      "Model saved to rl_portfolio_optimization.pth\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file_paths = [\n",
    "        r'training/bpcl1100d.xlsx',\n",
    "        r'training/hindpetro1100d.xlsx',\n",
    "        r'training/nmdc1100d.xlsx',\n",
    "        r'training/ongc1100d.xlsx',\n",
    "        r'training/irfc1100d.xlsx',\n",
    "        r'training/ioc1100d.xlsx',\n",
    "\n",
    "    ]\n",
    "    seq_len = 10\n",
    "    sequences = load_and_preprocess_data(file_paths, seq_len=seq_len)\n",
    "\n",
    "    num_assets = len(file_paths)\n",
    "    model = PortfolioOptimization(num_assets=num_assets, num_features=4, hidden_dim=128, input_seq_len=seq_len - 1).to(device)\n",
    "    train_model(model, sequences, num_epochs=5)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_input = sequences[0:1, :-1]\n",
    "        x_pred, weights = model(test_input)\n",
    "        print(\"Predicted Prices (Open, Close, High, Low):\", x_pred.cpu().numpy())\n",
    "        print(\"Portfolio Weights:\", weights[0].cpu().numpy())\n",
    "    torch.save(model.state_dict(), 'rl_portfolio_optimization.pth')\n",
    "    print(\"Model saved to rl_portfolio_optimization.pth\")\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de988a6-d123-4288-b88d-fc6dd941c89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ae81f-baf9-465c-939f-f63f476fc79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2edc0-238f-4f53-a992-ceafe2fed46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "010a2bad-6ef2-48a8-98f6-4e638dc82779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\style\\core.py:129\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:903\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[1;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[0;32m    902\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_or_url(fname) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:880\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    879\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n\u001b[1;32m--> 880\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 220\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaxNLocator\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Add this after the imports\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseaborn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    221\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.figsize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Modified Training Loop with Visualization\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\style\\core.py:131\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    129\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    135\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import logging\n",
    "from entmax import sparsemax  # Make sure you have installed: pip install entmax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()  # This will set default Seaborn style\n",
    "\n",
    "plt.style.use('seaborn-v0_8')  # or 'ggplot', 'bmh', etc.\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.basicConfig(filename='portfolio_optimization.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Data Preprocessing\n",
    "def load_and_preprocess_data(file_paths, seq_len=10, date_column='Date', feature_columns=['Open', 'Close', 'High', 'Low']):\n",
    "    dfs = []\n",
    "    for file in file_paths:\n",
    "        df = pd.read_excel(file)\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        required_columns = [date_column.lower()] + [col.lower() for col in feature_columns]\n",
    "\n",
    "        def convert_date(date):\n",
    "            if isinstance(date, (int, float)):\n",
    "                base_date = datetime(1899, 12, 30)\n",
    "                return pd.to_datetime(base_date + timedelta(days=date))\n",
    "            elif isinstance(date, pd.Timestamp):\n",
    "                return date\n",
    "            else:\n",
    "                return pd.to_datetime(date, errors='coerce')\n",
    "\n",
    "        df['date'] = df['date'].apply(convert_date)\n",
    "        df = df.set_index('date')\n",
    "\n",
    "        for col in feature_columns:\n",
    "            df[col.lower()] = df[col.lower()].round(2)\n",
    "\n",
    "        if 'volume' in df.columns:\n",
    "            df = df[df['volume'] != 0]\n",
    "            df = df.drop(columns=['volume'])\n",
    "\n",
    "        df = df[required_columns[1:]].dropna()\n",
    "        dfs.append(df)\n",
    "\n",
    "    common_dates = pd.concat([df[['close']] for df in dfs], axis=1).dropna().index\n",
    "    dfs = [df.loc[common_dates] for df in dfs]\n",
    "\n",
    "    data = np.stack([df[[col.lower() for col in feature_columns]].values for df in dfs], axis=1)\n",
    "    sequences = [data[i:i + seq_len] for i in range(len(data) - seq_len + 1)]\n",
    "    sequences = torch.tensor(np.array(sequences), dtype=torch.float32).to(device)\n",
    "    print(f\"Final sequence tensor shape: {sequences.shape}\")\n",
    "    return sequences\n",
    "\n",
    "# Dilated Causal Convolution Layer\n",
    "class DCC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
    "        super(DCC, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                              padding=(kernel_size - 1) * dilation, dilation=dilation)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        padding = (self.kernel_size - 1) * self.dilation\n",
    "        x = x[:, :, :-padding] if padding > 0 else x\n",
    "        return F.relu(x)\n",
    "\n",
    "# Graph Attention Layer\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.1):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.Wg = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.Wa = nn.Linear(2 * in_features, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, node_features, edge_info):\n",
    "        batch_size, num_nodes, in_features = node_features.size()\n",
    "        h = self.Wg(node_features)\n",
    "\n",
    "        attention = torch.zeros(batch_size, num_nodes, num_nodes, device=node_features.device)\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                concat_features = torch.cat([node_features[:, i, :], node_features[:, j, :]], dim=-1)\n",
    "                gate = torch.tanh(self.Wa(concat_features))\n",
    "                attention[:, i, j] = edge_info[:, i, j] * gate.squeeze()\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        h_prime = torch.bmm(attention, h)\n",
    "        return F.elu(h_prime)\n",
    "\n",
    "# Portfolio Optimization Model\n",
    "class PortfolioOptimization(nn.Module):\n",
    "    def __init__(self, num_assets, num_features=4, hidden_dim=64, input_seq_len=9):\n",
    "        super(PortfolioOptimization, self).__init__()\n",
    "        self.num_assets = num_assets\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_seq_len = input_seq_len\n",
    "        \n",
    "        # Feature extraction\n",
    "        self.dcc1 = DCC(num_features, hidden_dim, kernel_size=3, dilation=1)\n",
    "        self.dcc2 = DCC(hidden_dim, hidden_dim, kernel_size=3, dilation=2)\n",
    "        self.dcc3 = DCC(hidden_dim, hidden_dim, kernel_size=3, dilation=4)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.Wq = nn.Linear(num_features * input_seq_len, hidden_dim)\n",
    "        self.Wk = nn.Linear(num_features * input_seq_len, hidden_dim)\n",
    "        self.Wv = nn.Linear(num_features * input_seq_len, hidden_dim)\n",
    "\n",
    "        # Graph attention\n",
    "        self.gat = GATLayer(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Feature combination\n",
    "        self.Wr = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.We = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Prediction head\n",
    "        self.dcc_pred = DCC(hidden_dim * 3, num_features, kernel_size=3, dilation=1)\n",
    "\n",
    "        # Policy head\n",
    "        self.Wf = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.conv_policy = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.Wt = nn.Linear(hidden_dim + num_features, hidden_dim)\n",
    "        self.Ww = nn.Linear(hidden_dim, num_assets)\n",
    "\n",
    "        # Allocation parameters\n",
    "        self.min_weight = 0.05\n",
    "        self.rank_power = 1.0\n",
    "        self.temperature = nn.Parameter(torch.tensor(1.0))  # Learnable temperature\n",
    "        self.price_scale = nn.Parameter(torch.ones(num_features))\n",
    "        self.price_bias = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x, prev_weights=None):\n",
    "       \n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        batch_size, seq_len, num_assets, num_features = x.size()\n",
    "    \n",
    "        # Feature extraction\n",
    "        x_reshaped = x.permute(0, 2, 3, 1).reshape(batch_size * num_assets, num_features, seq_len)\n",
    "        fe = self.dcc1(x_reshaped)\n",
    "        fe = self.dcc2(fe)\n",
    "        fe = self.dcc3(fe)\n",
    "        fe = fe[:, :, -1].reshape(batch_size, num_assets, self.hidden_dim)\n",
    "    \n",
    "        # Cross-asset attention\n",
    "        patches_flat = x.reshape(batch_size, num_assets, -1)\n",
    "        q = self.Wq(patches_flat)\n",
    "        k = self.Wk(patches_flat)\n",
    "        v = self.Wv(patches_flat)\n",
    "        attention = torch.bmm(q, k.transpose(1, 2)) / (self.hidden_dim ** 0.5)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "    \n",
    "        # Graph attention\n",
    "        fr = self.gat(fe, attention)\n",
    "    \n",
    "        # Market context\n",
    "        fm = self.Wr(fr) + self.We(fe)\n",
    "        fm = fm.mean(dim=1, keepdim=True).expand(-1, num_assets, -1)\n",
    "    \n",
    "        # Combined features\n",
    "        f = torch.cat([fe, fr, fm], dim=-1)\n",
    "        f_reshaped = f.permute(0, 2, 1)\n",
    "        x_pred = self.dcc_pred(f_reshaped)\n",
    "        x_pred = x_pred.permute(0, 2, 1)\n",
    "        \n",
    "        # Ensure positive predictions using modified softplus\n",
    "        x_pred = F.softplus(x_pred) * 1.5  # Scale to prevent predictions from being too small\n",
    "    \n",
    "        # Policy features\n",
    "        f_policy = F.relu(self.conv_policy(self.Wf(f).permute(0, 2, 1))).permute(0, 2, 1)\n",
    "        f_policy = self.Wt(torch.cat([f_policy, x_pred], dim=-1))\n",
    "        \n",
    "        if prev_weights is not None:\n",
    "            f_policy = f_policy + prev_weights.unsqueeze(-1)\n",
    "    \n",
    "        # Raw score for allocation\n",
    "        raw_scores = self.Ww(f_policy[:, -1])  # shape: [B, A]\n",
    "        \n",
    "        # Get predicted returns from the Close prices (index 1 in features)\n",
    "        predicted_returns = x_pred[:, :, 1]  # shape: [B, A]\n",
    "        \n",
    "        # Combine raw scores with predicted returns for ranking\n",
    "        combined_scores = raw_scores + predicted_returns.detach()  # Detach to prevent double counting\n",
    "        \n",
    "        # Create differentiable rank weights\n",
    "        ranks = torch.argsort(torch.argsort(combined_scores, dim=1, descending=True))\n",
    "        rank_weights = 1.0 / (ranks.float() + 1)  # 1/rank weighting\n",
    "        \n",
    "        # Normalize rank weights\n",
    "        rank_weights = rank_weights / rank_weights.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Apply temperature-scaled sparsemax to combined scores\n",
    "        scaled_scores = combined_scores / (self.temperature + 1e-8)\n",
    "        sparse_weights = sparsemax(scaled_scores, dim=-1)\n",
    "        \n",
    "        # Blend sparse weights with rank weights (adjust ratio as needed)\n",
    "        weights = 0.7 * sparse_weights + 0.3 * rank_weights\n",
    "        \n",
    "        # Apply min allocation constraint and normalize\n",
    "        weights = self.min_weight + (1.0 - self.min_weight * num_assets) * weights\n",
    "        weights = weights / weights.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        return x_pred, weights\n",
    "# Training Loop\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Add this after the imports\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Modified Training Loop with Visualization\n",
    "def train_model(model, sequences, num_epochs=15, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    prev_weights = None\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    train_losses = []\n",
    "    train_returns = []\n",
    "    pred_losses = []\n",
    "    weight_entropies = []\n",
    "    max_weights = []\n",
    "    min_weights = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_return = 0\n",
    "        epoch_pred_loss = 0\n",
    "        epoch_entropy = 0\n",
    "        epoch_max_weight = 0\n",
    "        epoch_min_weight = 0\n",
    "\n",
    "        for batch in sequences:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x_input = batch[:-1].unsqueeze(0)\n",
    "            target = batch[1:].unsqueeze(0)[:, -1]\n",
    "\n",
    "            x_pred, weights = model(x_input, prev_weights)\n",
    "\n",
    "            # Calculate metrics\n",
    "            with torch.no_grad():\n",
    "                # Price prediction loss\n",
    "                pred_loss = F.mse_loss(x_pred, target)\n",
    "                \n",
    "                # Portfolio metrics\n",
    "                price_ratios = target[:, :, 1] / batch[-2, :, 1]  # Close-to-close returns\n",
    "                portfolio_return = torch.sum(price_ratios * weights, dim=-1)\n",
    "                \n",
    "                # Transaction costs\n",
    "                if prev_weights is not None:\n",
    "                    turnover = torch.sum(torch.abs(weights - prev_weights), dim=-1)\n",
    "                    transaction_costs = 0.0003 * turnover\n",
    "                    net_return = portfolio_return - transaction_costs\n",
    "                else:\n",
    "                    net_return = portfolio_return\n",
    "                \n",
    "                # Weight statistics\n",
    "                entropy = -torch.sum(weights * torch.log(weights + 1e-8), dim=-1).mean()\n",
    "                max_weight = weights.max(dim=-1)[0].mean()\n",
    "                min_weight = weights.min(dim=-1)[0].mean()\n",
    "\n",
    "            # Combined loss\n",
    "            return_loss = -torch.mean(torch.log(net_return + 1e-6))\n",
    "            loss = return_loss + 0.1 * pred_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_return += torch.mean(net_return).item()\n",
    "            epoch_pred_loss += pred_loss.item()\n",
    "            epoch_entropy += entropy.item()\n",
    "            epoch_max_weight += max_weight.item()\n",
    "            epoch_min_weight += min_weight.item()\n",
    "            \n",
    "            prev_weights = weights.detach()\n",
    "\n",
    "        # Store epoch metrics\n",
    "        n_batches = len(sequences)\n",
    "        train_losses.append(epoch_loss / n_batches)\n",
    "        train_returns.append(epoch_return / n_batches)\n",
    "        pred_losses.append(epoch_pred_loss / n_batches)\n",
    "        weight_entropies.append(epoch_entropy / n_batches)\n",
    "        max_weights.append(epoch_max_weight / n_batches)\n",
    "        min_weights.append(epoch_min_weight / n_batches)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Total Loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"  Avg Return: {train_returns[-1]:.4f}\")\n",
    "        print(f\"  Pred Loss: {pred_losses[-1]:.4f}\")\n",
    "        print(f\"  Weight Entropy: {weight_entropies[-1]:.4f}\")\n",
    "        print(f\"  Max Weight: {max_weights[-1]:.4f}, Min Weight: {min_weights[-1]:.4f}\")\n",
    "\n",
    "    # Plotting functions\n",
    "    def plot_metrics():\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "        \n",
    "        # Loss curves\n",
    "        ax1.plot(train_losses, label='Total Loss', color='tab:blue')\n",
    "        ax1.plot(pred_losses, label='Prediction Loss', color='tab:orange', linestyle='--')\n",
    "        ax1.set_title('Training Losses')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        \n",
    "        # Returns\n",
    "        ax2.plot(train_returns, label='Portfolio Return', color='tab:green')\n",
    "        ax2.set_title('Portfolio Returns')\n",
    "        ax2.set_ylabel('Return')\n",
    "        ax2.grid(True)\n",
    "        ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        \n",
    "        # Weight statistics\n",
    "        ax3.plot(max_weights, label='Max Weight', color='tab:red')\n",
    "        ax3.plot(min_weights, label='Min Weight', color='tab:purple')\n",
    "        ax3.plot(weight_entropies, label='Weight Entropy', color='tab:brown')\n",
    "        ax3.set_title('Weight Distribution Metrics')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Value')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "        ax3.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot final weights for last batch\n",
    "    def plot_final_weights(weights):\n",
    "        assets = ['HINDPETRO', 'NMDC', 'ONGC', 'IRFC', 'IOC']\n",
    "        weights = weights[0].cpu().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(assets, weights, color='skyblue')\n",
    "        plt.title('Final Portfolio Weights')\n",
    "        plt.ylabel('Allocation Percentage')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.2%}',\n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "        plt.grid(axis='y', linestyle='--')\n",
    "        plt.savefig('final_weights.png')\n",
    "        plt.show()\n",
    "    \n",
    "    # Generate plots\n",
    "    plot_metrics()\n",
    "    \n",
    "    # Plot weights from last batch\n",
    "    with torch.no_grad():\n",
    "        test_input = sequences[0:1, :-1]\n",
    "        _, final_weights = model(test_input)\n",
    "        plot_final_weights(final_weights)\n",
    "    \n",
    "    return {\n",
    "        'losses': train_losses,\n",
    "        'returns': train_returns,\n",
    "        'pred_losses': pred_losses,\n",
    "        'entropies': weight_entropies,\n",
    "        'max_weights': max_weights,\n",
    "        'min_weights': min_weights\n",
    "    }\n",
    "\n",
    "# Modified Main Execution\n",
    "def main():\n",
    "    file_paths = [\n",
    "        r'training/hindpetro1100d.xlsx',\n",
    "        r'training/nmdc1100d.xlsx',\n",
    "        r'training/ongc1100d.xlsx',\n",
    "        r'training/irfc1100d.xlsx',\n",
    "        r'training/ioc1100d.xlsx'\n",
    "    ]\n",
    "\n",
    "    seq_len = 10\n",
    "    sequences = load_and_preprocess_data(file_paths, seq_len=seq_len)\n",
    "    num_assets = len(file_paths)\n",
    "\n",
    "    model = PortfolioOptimization(num_assets=num_assets, num_features=4, hidden_dim=128, input_seq_len=seq_len-1).to(device)\n",
    "    \n",
    "    # Train and get metrics\n",
    "    metrics = train_model(model, sequences, num_epochs=5)  # Increased epochs for better plots\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_input = sequences[0:1, :-1]\n",
    "        x_pred, weights = model(test_input)\n",
    "        print(\"\\nFinal Predictions and Weights:\")\n",
    "        print(\"Predicted Prices:\", x_pred[0].cpu().numpy())\n",
    "        print(\"Portfolio Weights:\", weights[0].cpu().numpy())\n",
    "\n",
    "    torch.save(model.state_dict(), 'rl_portfolio_optimization.pth')\n",
    "    print(\"\\nModel saved to rl_portfolio_optimization.pth\")\n",
    "\n",
    "    # Plot predicted prices\n",
    "    def plot_predicted_prices(preds):\n",
    "        assets = ['HINDPETRO', 'NMDC', 'ONGC', 'IRFC', 'IOC']\n",
    "        features = ['Open', 'Close', 'High', 'Low']\n",
    "        preds = preds[0].cpu().numpy()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (ax, feature) in enumerate(zip(axes, features)):\n",
    "            ax.bar(assets, preds[:, i], color=plt.cm.tab20(i))\n",
    "            ax.set_title(f'Predicted {feature} Prices')\n",
    "            ax.set_ylabel('Price')\n",
    "            ax.grid(True)\n",
    "            \n",
    "            # Add value labels\n",
    "            for j, asset in enumerate(assets):\n",
    "                ax.text(j, preds[j, i], f'{preds[j, i]:.2f}',\n",
    "                       ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('predicted_prices.png')\n",
    "        plt.show()\n",
    "    \n",
    "    plot_predicted_prices(x_pred)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4433cdd-5f29-464e-a022-bad44b6058a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535aeb6-6782-43c1-90fe-3207da4e33b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df8501-8665-4c8b-aff0-b2224aaf9a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
